{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yerdaulet/Documents/bee/VAE/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm \n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations (normalize pixel values to [-1, 1])\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    # transforms.Normalize((0.5,), (0.5,))  # Normalize with mean=0.5 and std=0.5\n",
    "])\n",
    "\n",
    "# Download and load the training and test datasets\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./data',  # Directory to save the dataset\n",
    "    train=True,     # Load the training set\n",
    "    transform=transform,\n",
    "    download=True   # Download the dataset if not already available\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,    # Load the test set\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Data loaders for batch processing\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,  # Batch size\n",
    "    shuffle=True    # Shuffle the data for training\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False   # No need to shuffle for testing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Auto Encoder's Theory\n",
    "\n",
    "\n",
    "### Resources: \n",
    "https://arxiv.org/pdf/1312.6114\n",
    "\n",
    "https://www.youtube.com/watch?v=YV9D3TWY5Zo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/1.png\" alt=\"Image\" width=\"700\" height=\"300\", title=\"VAE\">\n",
    "<br>\n",
    "<label><h5>High reconstruction error indicates an anomaly, <br>meaning the input data is different from the data the network has seen during training</h5></label>\n",
    "<br>\n",
    "<img src=\"images/2.png\" alt=\"Image\" width=\"700\" height=\"300\", title=\"VAE\">\n",
    "<br>\n",
    "\n",
    "<label><h5>The left graph represents a normal AE, while the right one represents a VAE.</h5></label>\n",
    "</center>\n",
    "Normal AE is not robust when the image is slightly different from the data it has seen during training. <br>\n",
    "It has an unknown distribution of the latent space, and there is no possibility of smooth transformation of the reconstruction. <br>\n",
    "If the decoder receives a noisy latent vector, it returns noise as well. <br>\n",
    "Working with an unknown distribution is extremely difficult, <br>\n",
    "so we need to regularize the latent vector distribution to a well-known Gaussian distribution using KL divergence (two distribution similarity score)\n",
    "</label>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"images/3.png\" alt=\"Image\" width=\"800\" height=\"400\", title=\"VAE\">\n",
    "<br>\n",
    "</center>\n",
    "<center>\n",
    "<img src=\"images/4.png\" alt=\"Image\" width=\"600\" height=\"300\", title=\"VAE\">\n",
    "<br>\n",
    "<h5>Issue: Backpropagation through a randomized latent vector from a Gaussian distribution <br>with learned mean and standard deviation values from the encoder <br></h5>\n",
    "</center>\n",
    "\n",
    "<center><h5>Reparametrization trick solves the backpropogation issue</h5>\n",
    "<img src=\"images/5.png\" alt=\"Image\" width=\"700\" height=\"400\", title=\"VAE\">\n",
    "\n",
    "<label>Encoder needs to learn to describe the mean and std of a latent vector</label>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"images/6.png\" alt=\"Image\" width=\"800\" height=\"800\", title=\"Paper\">\n",
    "\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss function: Reconstruction loss + KL divergence loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder network\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            # nn.Conv2d(128, 1, kernel_size=4, stride=2, padding=1), \n",
    "            # nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(27, 2)\n",
    "        self.fc_logvar = nn.Linear(27, 2)\n",
    "\n",
    "        # Decoder network88\n",
    "        self.fc_decode = nn.Linear(2, 27)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (3, 3, 3)),\n",
    "            nn.ConvTranspose2d(3, 128, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=0),  \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),    \n",
    "            # nn.ReLU(),\n",
    "            # nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),     \n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = self.encoder(x)\n",
    "        mu = self.fc_mu(h1)\n",
    "        logvar = self.fc_logvar(h1)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Convert logvar to standard deviation (std)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.fc_decode(z)\n",
    "        x_reconstructed = self.decoder(h3)\n",
    "        return x_reconstructed\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_reconstructed = self.decode(z)\n",
    "        return x_reconstructed, mu, logvar\n",
    "\n",
    "def loss_function(reconstructed_x, x, mu, logvar):\n",
    "    # Binary Cross-Entropy (BCE) loss for reconstruction + KL divergence loss\n",
    "    BCE = F.binary_cross_entropy(reconstructed_x, x, reduction='sum')\n",
    "    # MSE = F.mse_loss(input=reconstructed_x, target=x, reduction='sum')\n",
    "    # KL Divergence losssum\n",
    "    # Equation for KL divergence between normal distribution and learned distribution\n",
    "    # D_KL = -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    # where sigma is exp(logvar / 2)\n",
    "    # We are working with the N(0,1) as the prior\n",
    "    # This loss is summed over all pixels in the image\n",
    "    KL_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KL_divergence\n",
    "\n",
    "\n",
    "# vae = VAE()\n",
    "# vae(imgs.cpu())[0].shape\n",
    "# input_image = torch.ones((1, 1, 28, 28))\n",
    "# vae(input_image)[0].shape\n",
    "# reconstructed_image, mu, logvar = vae(imgs[:])\n",
    "# loss_function(reconstructed_image, imgs[:], mu, logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models_mnist/\", exist_ok=True)\n",
    "\n",
    "def save_best_model(vae, lowest_val_loss, epoch, train_loss, val_loss):\n",
    "    torch.save({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": round(train_loss, 2),\n",
    "        \"val_loss\": round(val_loss, 2),\n",
    "        \"state_dict\": vae.state_dict(),\n",
    "    }, f\"models_mnist/last_model.pt\")\n",
    "    \n",
    "    if lowest_val_loss > val_loss:\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": round(train_loss, 2),\n",
    "            \"val_loss\": round(val_loss, 2),\n",
    "            \"state_dict\": vae.state_dict(),\n",
    "        }, f\"models_mnist/best_model_{epoch}_t_{train_loss}_v_{val_loss}.pt\")\n",
    "        print(f\"The best model was saved with val_loss: {val_loss}\")\n",
    "        return val_loss\n",
    "    return lowest_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:44<13:57, 44.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was saved with val_loss: 10818.302734375\n",
      "Epoch: 0 | training loss: 12200.50 | validation loss: 10818.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [01:18<11:30, 38.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was saved with val_loss: 10374.2783203125\n",
      "Epoch: 1 | training loss: 10637.01 | validation loss: 10374.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [01:52<10:21, 36.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was saved with val_loss: 10145.2119140625\n",
      "Epoch: 2 | training loss: 10311.78 | validation loss: 10145.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [02:26<09:28, 35.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was saved with val_loss: 10059.8857421875\n",
      "Epoch: 3 | training loss: 10140.46 | validation loss: 10059.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [03:01<08:47, 35.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was saved with val_loss: 9930.716796875\n",
      "Epoch: 4 | training loss: 10032.85 | validation loss: 9930.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [03:35<08:08, 34.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was saved with val_loss: 9926.8447265625\n",
      "Epoch: 5 | training loss: 9949.07 | validation loss: 9926.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [04:09<07:30, 34.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was saved with val_loss: 9875.947265625\n",
      "Epoch: 6 | training loss: 9887.83 | validation loss: 9875.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [04:44<06:54, 34.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was saved with val_loss: 9772.4912109375\n",
      "Epoch: 7 | training loss: 9829.07 | validation loss: 9772.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [05:18<06:17, 34.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was saved with val_loss: 9769.8701171875\n",
      "Epoch: 8 | training loss: 9788.48 | validation loss: 9769.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [05:52<05:44, 34.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was saved with val_loss: 9747.5712890625\n",
      "Epoch: 9 | training loss: 9750.28 | validation loss: 9747.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [06:26<05:09, 34.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was saved with val_loss: 9730.9072265625\n",
      "Epoch: 10 | training loss: 9717.52 | validation loss: 9730.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [07:00<04:34, 34.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was saved with val_loss: 9633.6318359375\n",
      "Epoch: 11 | training loss: 9690.03 | validation loss: 9633.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [07:34<03:59, 34.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | training loss: 9663.61 | validation loss: 9691.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [08:08<03:24, 34.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was saved with val_loss: 9632.2861328125\n",
      "Epoch: 13 | training loss: 9643.19 | validation loss: 9632.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [08:43<02:52, 34.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | training loss: 9624.30 | validation loss: 9636.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [09:18<02:17, 34.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was saved with val_loss: 9617.248046875\n",
      "Epoch: 15 | training loss: 9602.41 | validation loss: 9617.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [09:52<01:43, 34.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | training loss: 9585.33 | validation loss: 9739.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [10:27<01:09, 34.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | training loss: 9569.87 | validation loss: 9636.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [11:02<00:34, 34.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was saved with val_loss: 9554.8623046875\n",
      "Epoch: 18 | training loss: 9552.82 | validation loss: 9554.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [11:36<00:00, 34.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model was saved with val_loss: 9537.3115234375\n",
      "Epoch: 19 | training loss: 9542.13 | validation loss: 9537.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "iterations = 20\n",
    "lowest_val_loss = torch.inf\n",
    "train_loss, val_loss = [], []\n",
    "\n",
    "\n",
    "for epoch in tqdm.trange(iterations):\n",
    "    vae.train()\n",
    "    epoch_loss, count = 0, 0\n",
    "    for imgs, _ in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        reconstructed_mel, mu, logvar = vae(imgs)\n",
    "        loss = loss_function(reconstructed_mel, imgs, mu, logvar)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss\n",
    "        count += 1  \n",
    "    loss = epoch_loss / count\n",
    "    train_loss.append(loss.item())\n",
    "\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_loss, count = 0, 0\n",
    "        for imgs, _ in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            reconstructed_mel, mu, logvar = vae(imgs)\n",
    "            loss = loss_function(reconstructed_mel, imgs, mu, logvar)\n",
    "            epoch_loss += loss\n",
    "            count += 1\n",
    "        loss = epoch_loss / count\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "    lowest_val_loss = save_best_model(vae, lowest_val_loss, epoch, train_loss[-1], val_loss[-1])\n",
    "    print(f\"Epoch: {epoch} | training loss: {train_loss[-1]:.2f} | validation loss: {val_loss[-1]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame({\n",
    "#     \"epoch\": range(len(train_loss)),\n",
    "#     \"train_loss\": train_loss,\n",
    "#     \"val_loss\": val_loss\n",
    "# }).to_csv('mnist_vae_loss.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58310/1659441775.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vae.load_state_dict(torch.load('models_mnist/best_model_19_t_9542.1279296875_v_9537.3115234375.pt')['state_dict'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.eval().cpu();\n",
    "vae.load_state_dict(torch.load('models_mnist/best_model_19_t_9542.1279296875_v_9537.3115234375.pt')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7be9e71dc800>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHxZJREFUeJzt3X9wVfW57/HPTkg2CSQ7hpBfEmhAlCqS3lKJGZViSQnpOQ4o0/FXZ8BxYKTBKVKrQ0dFbWfS4oz16FD947ZQZ8RfcwVGx9KjYMK1BXpBGUp/pCQnSigkCD3JToIkIft7/+AYuyWA38VOniS8XzNrhuy9nqwni2/yycpeeRJyzjkBADDIkqwbAABcmgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmBhl3cAXxWIxHTlyRBkZGQqFQtbtAAA8OefU3t6uwsJCJSWd+zpnyAXQkSNHVFRUZN0GAOAiNTU1acKECed8fsgFUEZGhiTpRn1Ho5Ri3A0AwNdp9eh9vd339fxcBiyA1q1bp6eeekrNzc0qKSnRc889p1mzZl2w7rMfu41SikaFCCAAGHb+Z8LohV5GGZCbEF599VWtWrVKa9as0QcffKCSkhJVVFTo2LFjA3E4AMAwNCAB9PTTT2vp0qW65557dPXVV+uFF15Qenq6fv3rXw/E4QAAw1DCA6i7u1t79+5VeXn55wdJSlJ5ebl27tx51v5dXV2KRqNxGwBg5Et4AB0/fly9vb3Ky8uLezwvL0/Nzc1n7V9dXa1IJNK3cQccAFwazH8RdfXq1Wpra+vbmpqarFsCAAyChN8Fl5OTo+TkZLW0tMQ93tLSovz8/LP2D4fDCofDiW4DADDEJfwKKDU1VTNnztS2bdv6HovFYtq2bZvKysoSfTgAwDA1IL8HtGrVKi1evFjf+MY3NGvWLD3zzDPq7OzUPffcMxCHAwAMQwMSQLfffrs++eQTPfbYY2pubtbXvvY1bd269awbEwAAl66Qc85ZN/GvotGoIpGI5mgBkxAAYBg67XpUoy1qa2tTZmbmOfczvwsOAHBpIoAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYGJApmEDuICkZP+SMeneNaG8HO8aSXLpAf5IZMy/JKm907vGdQSo6TzpXSNJ7vTpQam5VHEFBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwwTRs4GKFQv4lKf6fer3TJ3vXNFWM8a6RpFNF3d41ya3+H1P2ny7zr/lzh3dN8uFPvGskKfbfrd41rrfX/0DO+deMAFwBAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMMEwUuBihfy/j0vOHe9dc/C7ad41Zdf/xbtGkv7Zle5d89e/X+5dExs1SF+CkpMHrS4UoOZSHWDKFRAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATDCMF/lUo5F2SPHaMd83Rfy/yrln27Xe9ayalHveukaRnGuZ612T9KcW75rKDn3rXJJ9o965xnSe9ayRJAYaEutjwHxI6WLgCAgCYIIAAACYSHkCPP/64QqFQ3DZt2rREHwYAMMwNyGtA11xzjd599/OfV48arD86BQAYNgYkGUaNGqX8/PyBeNcAgBFiQF4DOnjwoAoLCzV58mTdfffdOnTo0Dn37erqUjQajdsAACNfwgOotLRUGzZs0NatW/X888+rsbFRN910k9rb+791srq6WpFIpG8rKvK/PRUAMPwkPIAqKyv13e9+VzNmzFBFRYXefvtttba26rXXXut3/9WrV6utra1va2pqSnRLAIAhaMDvDsjKytKVV16p+vr6fp8Ph8MKh8MD3QYAYIgZ8N8D6ujoUENDgwoKCgb6UACAYSThAfTggw+qtrZWH330kf7whz/o1ltvVXJysu68885EHwoAMIwl/Edwhw8f1p133qkTJ05o/PjxuvHGG7Vr1y6NHz8+0YcCAAxjCQ+gV155JdHvEhg0SQFej2ytvNq75s6q//Su+faYv3jXvNVe4l0jSZ01ud41+fv8B36mNp3wrnEdnf41XV3eNZLkHINFBxKz4AAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgY8D9IB1gIjQq4tK/4indJ6r3N3jX3RPZ71xw+7f8xbThwvXeNJE34U493TUpzm3dNoMGinf5DT11vzLsGA48rIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACaZhY+hLSvYvueyyQIc6XJHtXfO/p/6Hd83okP/H9NPD/+Zdk/9/wt41kjSmrsW7xrVF/Wu6/aduj0jOWXdggisgAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJhhGisEVCnmXJI32H6jZfU2Rd40kXbnw7941s8Ip3jWbO/2Hpda/fqV3zeV7D3vXSJLr6PQv6u31r4nFvEtckMGdSf7rLrAApyHI58VIGGDKFRAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATDCNFcAEGKIaSk71rksZle9d8dEuqd40kvT1xi3fN8QDDJx/8f0u8a674fZt3TaChopIU4P8pyHDMUCzAQM0gQ0+TBu97bddzOkDR8B8sGgRXQAAAEwQQAMCEdwDt2LFDt9xyiwoLCxUKhbR58+a4551zeuyxx1RQUKC0tDSVl5fr4MGDieoXADBCeAdQZ2enSkpKtG7dun6fX7t2rZ599lm98MIL2r17t8aMGaOKigqdOnXqopsFAIwc3jchVFZWqrKyst/nnHN65pln9Mgjj2jBggWSpBdffFF5eXnavHmz7rjjjovrFgAwYiT0NaDGxkY1NzervLy877FIJKLS0lLt3Lmz35quri5Fo9G4DQAw8iU0gJqbmyVJeXl5cY/n5eX1PfdF1dXVikQifVtRUVEiWwIADFHmd8GtXr1abW1tfVtTU5N1SwCAQZDQAMrPz5cktbS0xD3e0tLS99wXhcNhZWZmxm0AgJEvoQFUXFys/Px8bdu2re+xaDSq3bt3q6ysLJGHAgAMc953wXV0dKi+vr7v7cbGRu3bt0/Z2dmaOHGiVq5cqZ/+9KeaOnWqiouL9eijj6qwsFALFy5MZN8AgGHOO4D27Nmjm2++ue/tVatWSZIWL16sDRs26KGHHlJnZ6eWLVum1tZW3Xjjjdq6datGjx6duK4BAMOedwDNmTNH7jyD80KhkJ588kk9+eSTF9UYhr4gg0VDaWneNV1Tcr1rbrrhz941kjRplP8Q0+dbp3rXZO7wPw9Jn570rlFWsNdUXXrYuybUG2AYaVuHd406/GsCDQiVgg0+DSLAYN+RMMDU/C44AMCliQACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgwnsaNkagIJN4JYVG+S+fpLFjvGtavuH/pzyWZe/3rpGkj093e9f8pv5675pwu/8k466CDO+a3tRg32OeTvevSzrt/zGNrb/wPmfp6fEuCcWCTY52gzUN+xLFFRAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATDCMdaQIMFg2NSgl2qLQ075pYXrZ3zenSdu+a/FFt3jWStKV9hndNtCHLuyZjnP/3fifzwt41p/1nv/5Pnf/wzlGd/muvNzXLuyayP8CA0G7/AaYYeFwBAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMMEw0qFskAaLJo3xHyoqSRqX5V3yzxkR75pFU9/3rklVgIGVkpq7/PtzYf/Bne1fiXnXxCKnvWtS0oMN4QyP9q/rbB/tXdN+0r9m7Mf+6zX5xBD/Xtv5r6GRYIj/rwAARioCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmGEY6WAZrsGia/3BHjQq4DFL869qu8D9MecafvWsiSV3+B5LU0Rv2rnFp/oNPk7L8a0K9/mvodE+yd40kjUn3P3+RyEnvmvaCVO+a7sv8/4/SA65xlxzk/AUbAHsp4goIAGCCAAIAmPAOoB07duiWW25RYWGhQqGQNm/eHPf8kiVLFAqF4rb58+cnql8AwAjhHUCdnZ0qKSnRunXrzrnP/PnzdfTo0b7t5ZdfvqgmAQAjj/crc5WVlaqsrDzvPuFwWPn5+YGbAgCMfAPyGlBNTY1yc3N11VVXafny5Tpx4sQ59+3q6lI0Go3bAAAjX8IDaP78+XrxxRe1bds2/fznP1dtba0qKyvV29v/bafV1dWKRCJ9W1FRUaJbAgAMQQn/PaA77rij79/XXnutZsyYoSlTpqimpkZz5849a//Vq1dr1apVfW9Ho1FCCAAuAQN+G/bkyZOVk5Oj+vr6fp8Ph8PKzMyM2wAAI9+AB9Dhw4d14sQJFRQUDPShAADDiPeP4Do6OuKuZhobG7Vv3z5lZ2crOztbTzzxhBYtWqT8/Hw1NDTooYce0hVXXKGKioqENg4AGN68A2jPnj26+eab+97+7PWbxYsX6/nnn9f+/fv1m9/8Rq2trSosLNS8efP0k5/8ROGw//wmAMDI5R1Ac+bMkXPunM//7ne/u6iGRqpQgKGGoQDDPoMMCA3Kpfofq6uo27tmcor/rflHTqd510jSP05m+RcFGBLaG/UfNBv+JMAaOu3fmyS1TvA/VmZuh/+BArQXOveXn/MUBTsPOs/XOlw8ZsEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwM3ujkkSToZF1fsZh/Tc9p/5pU/8nMktQ7JtX/UOk93jXtMf/JzNHYaO+awHr810N6k/+nXubH/uuhJ9hQcHVN9j9Weth/0vnJAJPEe8b4r4e0tIDr4b9b/WtcgM/bSxRXQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwwjHSwhIZu1odGBxvU2Jvmv3xczH9Y6ie9Y7xrPuoZ710jSa2n/Kd3prQFGI553HnXhHq9S9Q5Idjg3BnFh71rugMMjW3t8O8v3Oo/0NZF271rJMmdDjDc1/n/316qhu5XRQDAiEYAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEw0gHiesNMEnSxRLfSH96/Ic7Bj5UR6p3zd+7871r8ke1etdI0g15/+Vd89q4cd41nZ+meNf8s8R/PVw9vdG7RpImjvmnd83vGr7qXZN90P9jSv1Hm3eN6+j0rpECft7iS+MKCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAmGkQbhXIAi/6GLbpDmIIa6gw0jTYl2e9ekN2Z417z7Vf8hlysKt3nXSNL/Sv/Yuya91P88/PnqAu+a4jEnvGuSA6w7SdrcMMO7Jv3/jvWuyfqz/9BTHTvuXRILuMaDfa7jy+IKCABgggACAJjwCqDq6mpdd911ysjIUG5urhYuXKi6urq4fU6dOqWqqiqNGzdOY8eO1aJFi9TS0pLQpgEAw59XANXW1qqqqkq7du3SO++8o56eHs2bN0+dnZ//sacHHnhAb775pl5//XXV1tbqyJEjuu222xLeOABgePO6CWHr1q1xb2/YsEG5ubnau3evZs+erba2Nv3qV7/Sxo0b9a1vfUuStH79en31q1/Vrl27dP311yeucwDAsHZRrwG1tZ3507jZ2dmSpL1796qnp0fl5eV9+0ybNk0TJ07Uzp07+30fXV1dikajcRsAYOQLHECxWEwrV67UDTfcoOnTp0uSmpublZqaqqysrLh98/Ly1Nzc3O/7qa6uViQS6duKioqCtgQAGEYCB1BVVZUOHDigV1555aIaWL16tdra2vq2pqami3p/AIDhIdAvoq5YsUJvvfWWduzYoQkTJvQ9np+fr+7ubrW2tsZdBbW0tCg/P7/f9xUOhxUOh4O0AQAYxryugJxzWrFihTZt2qTt27eruLg47vmZM2cqJSVF27Z9/lvodXV1OnTokMrKyhLTMQBgRPC6AqqqqtLGjRu1ZcsWZWRk9L2uE4lElJaWpkgkonvvvVerVq1Sdna2MjMzdf/996usrIw74AAAcbwC6Pnnn5ckzZkzJ+7x9evXa8mSJZKkX/ziF0pKStKiRYvU1dWliooK/fKXv0xIswCAkSPk3NCatheNRhWJRDRHCzQqlGLdzvCTlOxfkhrsPCeNz/GuOTU1z7um6dup3jUzbjzoXSNJ/z5+v3dNRtIp75r/6h7vXfOn9su9a96vm+pdI0mRP/q/Ljt+X+eFd/qCUXX+Nx3F2vx/VcP1BpzsO7S+PA4bp12ParRFbW1tyszMPOd+zIIDAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgI9BdRMYS5mHdJrLsn2LE+Oe5dEj550rvmimb/qdtHP5jiXSNJvyi80rumo9j/nI/qDHnXjD7uXzN5n/+kbkkKf/QP75rYMf/10BtgPTCheuTgCggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJhpGONEEGNbreQIeKdfkP4VSAwaehaId3TebH/sM0JSmSNtq7JjTav8b1BBgA29XlXRLr/NT/OJJ6Awy1db0B1hGDRS9pXAEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwwTBSBDdIg09dLEDN6QDDPiXp5MlgdYOBwZ0YYbgCAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIJhpBiZGNwJDHlcAQEATBBAAAATXgFUXV2t6667ThkZGcrNzdXChQtVV1cXt8+cOXMUCoXitvvuuy+hTQMAhj+vAKqtrVVVVZV27dqld955Rz09PZo3b546Ozvj9lu6dKmOHj3at61duzahTQMAhj+vmxC2bt0a9/aGDRuUm5urvXv3avbs2X2Pp6enKz8/PzEdAgBGpIt6DaitrU2SlJ2dHff4Sy+9pJycHE2fPl2rV6/WyfP8meOuri5Fo9G4DQAw8gW+DTsWi2nlypW64YYbNH369L7H77rrLk2aNEmFhYXav3+/Hn74YdXV1emNN97o9/1UV1friSeeCNoGAGCYCjkX7Bcmli9frt/+9rd6//33NWHChHPut337ds2dO1f19fWaMmXKWc93dXWpq6ur7+1oNKqioiLN0QKNCqUEaQ0AYOi061GNtqitrU2ZmZnn3C/QFdCKFSv01ltvaceOHecNH0kqLS2VpHMGUDgcVjgcDtIGAGAY8wog55zuv/9+bdq0STU1NSouLr5gzb59+yRJBQUFgRoEAIxMXgFUVVWljRs3asuWLcrIyFBzc7MkKRKJKC0tTQ0NDdq4caO+853vaNy4cdq/f78eeOABzZ49WzNmzBiQDwAAMDx5vQYUCoX6fXz9+vVasmSJmpqa9L3vfU8HDhxQZ2enioqKdOutt+qRRx45788B/1U0GlUkEuE1IAAYpgbkNaALZVVRUZFqa2t93iUA4BLFLDgAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgIlR1g18kXNOknRaPZIzbgYA4O20eiR9/vX8XIZcALW3t0uS3tfbxp0AAC5Ge3u7IpHIOZ8PuQtF1CCLxWI6cuSIMjIyFAqF4p6LRqMqKipSU1OTMjMzjTq0x3k4g/NwBufhDM7DGUPhPDjn1N7ersLCQiUlnfuVniF3BZSUlKQJEyacd5/MzMxLeoF9hvNwBufhDM7DGZyHM6zPw/mufD7DTQgAABMEEADAxLAKoHA4rDVr1igcDlu3YorzcAbn4QzOwxmchzOG03kYcjchAAAuDcPqCggAMHIQQAAAEwQQAMAEAQQAMDFsAmjdunX6yle+otGjR6u0tFR//OMfrVsadI8//rhCoVDcNm3aNOu2BtyOHTt0yy23qLCwUKFQSJs3b4573jmnxx57TAUFBUpLS1N5ebkOHjxo0+wAutB5WLJkyVnrY/78+TbNDpDq6mpdd911ysjIUG5urhYuXKi6urq4fU6dOqWqqiqNGzdOY8eO1aJFi9TS0mLU8cD4Mudhzpw5Z62H++67z6jj/g2LAHr11Ve1atUqrVmzRh988IFKSkpUUVGhY8eOWbc26K655hodPXq0b3v//fetWxpwnZ2dKikp0bp16/p9fu3atXr22Wf1wgsvaPfu3RozZowqKip06tSpQe50YF3oPEjS/Pnz49bHyy+/PIgdDrza2lpVVVVp165deuedd9TT06N58+aps7Ozb58HHnhAb775pl5//XXV1tbqyJEjuu222wy7Trwvcx4kaenSpXHrYe3atUYdn4MbBmbNmuWqqqr63u7t7XWFhYWuurrasKvBt2bNGldSUmLdhilJbtOmTX1vx2Ixl5+f75566qm+x1pbW104HHYvv/yyQYeD44vnwTnnFi9e7BYsWGDSj5Vjx445Sa62ttY5d+b/PiUlxb3++ut9+/z1r391ktzOnTut2hxwXzwPzjn3zW9+0/3gBz+wa+pLGPJXQN3d3dq7d6/Ky8v7HktKSlJ5ebl27txp2JmNgwcPqrCwUJMnT9bdd9+tQ4cOWbdkqrGxUc3NzXHrIxKJqLS09JJcHzU1NcrNzdVVV12l5cuX68SJE9YtDai2tjZJUnZ2tiRp79696unpiVsP06ZN08SJE0f0evjiefjMSy+9pJycHE2fPl2rV6/WyZMnLdo7pyE3jPSLjh8/rt7eXuXl5cU9npeXp7/97W9GXdkoLS3Vhg0bdNVVV+no0aN64okndNNNN+nAgQPKyMiwbs9Ec3OzJPW7Pj577lIxf/583XbbbSouLlZDQ4N+/OMfq7KyUjt37lRycrJ1ewkXi8W0cuVK3XDDDZo+fbqkM+shNTVVWVlZcfuO5PXQ33mQpLvuukuTJk1SYWGh9u/fr4cfflh1dXV64403DLuNN+QDCJ+rrKzs+/eMGTNUWlqqSZMm6bXXXtO9995r2BmGgjvuuKPv39dee61mzJihKVOmqKamRnPnzjXsbGBUVVXpwIEDl8TroOdzrvOwbNmyvn9fe+21Kigo0Ny5c9XQ0KApU6YMdpv9GvI/gsvJyVFycvJZd7G0tLQoPz/fqKuhISsrS1deeaXq6+utWzHz2RpgfZxt8uTJysnJGZHrY8WKFXrrrbf03nvvxf35lvz8fHV3d6u1tTVu/5G6Hs51HvpTWloqSUNqPQz5AEpNTdXMmTO1bdu2vsdisZi2bdumsrIyw87sdXR0qKGhQQUFBdatmCkuLlZ+fn7c+ohGo9q9e/clvz4OHz6sEydOjKj14ZzTihUrtGnTJm3fvl3FxcVxz8+cOVMpKSlx66Gurk6HDh0aUevhQuehP/v27ZOkobUerO+C+DJeeeUVFw6H3YYNG9xf/vIXt2zZMpeVleWam5utWxtUP/zhD11NTY1rbGx0v//97115ebnLyclxx44ds25tQLW3t7sPP/zQffjhh06Se/rpp92HH37oPv74Y+eccz/72c9cVlaW27Jli9u/f79bsGCBKy4udp9++qlx54l1vvPQ3t7uHnzwQbdz507X2Njo3n33Xff1r3/dTZ061Z06dcq69YRZvny5i0Qirqamxh09erRvO3nyZN8+9913n5s4caLbvn2727NnjysrK3NlZWWGXSfehc5DfX29e/LJJ92ePXtcY2Oj27Jli5s8ebKbPXu2cefxhkUAOefcc8895yZOnOhSU1PdrFmz3K5du6xbGnS33367KygocKmpqe7yyy93t99+u6uvr7dua8C99957TtJZ2+LFi51zZ27FfvTRR11eXp4Lh8Nu7ty5rq6uzrbpAXC+83Dy5Ek3b948N378eJeSkuImTZrkli5dOuK+Sevv45fk1q9f37fPp59+6r7//e+7yy67zKWnp7tbb73VHT161K7pAXCh83Do0CE3e/Zsl52d7cLhsLviiivcj370I9fW1mbb+Bfw5xgAACaG/GtAAICRiQACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgIn/DyfVyF1AkrutAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = vae.decode(torch.tensor(([[0.7, 0.8]]), dtype=torch.float32, device='cpu'))\n",
    "plt.imshow(image.detach().cpu().squeeze(0).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out, m, std = vae(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7be9d45a2000>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(out[0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0667)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(input=out[0], target=imgs[0], reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
